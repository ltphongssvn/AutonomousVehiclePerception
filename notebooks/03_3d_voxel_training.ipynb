{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - 3D Voxel CNN Training\n",
    "# AutonomousVehiclePerception/notebooks/03_3d_voxel_training.ipynb\n",
    "\n",
    "Train 3D CNN on voxelized LiDAR point clouds for BEV object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from src.model.cnn_3d_voxel import VoxelBackbone3D\n",
    "from src.data.kitti_dataset import voxelize_points\n",
    "from src.data.augmentations import get_lidar_augmentations\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'num_classes': 5,\n",
    "    'voxel_size': (0.2, 0.2, 0.2),\n",
    "    'point_range': (-40, -40, -3, 40, 40, 1),\n",
    "    'batch_size': 4,\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'val_split': 0.2,\n",
    "    'kitti_root': '../data/raw/kitti',\n",
    "    'checkpoint_dir': '../checkpoints',\n",
    "    'log_dir': '../logs/tensorboard/voxel3d',\n",
    "}\n",
    "\n",
    "Path(CONFIG['checkpoint_dir']).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG['log_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Voxelize LiDAR point clouds into dense 3D grids for Conv3d processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_root = Path(CONFIG['kitti_root'])\n",
    "lidar_dir = kitti_root / 'training' / 'velodyne'\n",
    "\n",
    "if lidar_dir.exists():\n",
    "    from src.data.kitti_dataset import load_lidar_points\n",
    "    bin_files = sorted(lidar_dir.glob('*.bin'))[:200]\n",
    "    print(f'Loading {len(bin_files)} LiDAR scans...')\n",
    "\n",
    "    voxel_grids = []\n",
    "    for bf in bin_files:\n",
    "        pts = load_lidar_points(str(bf))\n",
    "        vg = voxelize_points(pts, CONFIG['voxel_size'], CONFIG['point_range'])\n",
    "        voxel_grids.append(vg)\n",
    "\n",
    "    X = torch.from_numpy(np.stack(voxel_grids)).unsqueeze(1)  # (N, 1, D, H, W)\n",
    "    # Placeholder labels for BEV grid\n",
    "    Y = torch.randint(0, CONFIG['num_classes'], (len(voxel_grids), X.shape[3]//4, X.shape[4]//4))\n",
    "else:\n",
    "    print('KITTI LiDAR not found. Using synthetic voxel data.')\n",
    "    N = 100\n",
    "    D, H, W = 20, 128, 128\n",
    "    X = torch.randn(N, 1, D, H, W)\n",
    "    Y = torch.randint(0, CONFIG['num_classes'], (N, H // 4, W // 4))\n",
    "\n",
    "print(f'Voxel data shape: {X.shape}')\n",
    "print(f'Labels shape: {Y.shape}')\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "val_size = int(len(dataset) * CONFIG['val_split'])\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, pin_memory=True)\n",
    "print(f'Train: {train_size}, Val: {val_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiDAR Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_augs = get_lidar_augmentations()\n",
    "print('Available LiDAR augmentations:')\n",
    "for name in lidar_augs:\n",
    "    print(f'  - {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VoxelBackbone3D(in_channels=1, num_classes=CONFIG['num_classes']).to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'VoxelBackbone3D parameters: {num_params:,}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'])\n",
    "writer = SummaryWriter(log_dir=CONFIG['log_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    start = time.time()\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for voxels, targets in train_loader:\n",
    "        voxels, targets = voxels.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(voxels)\n",
    "\n",
    "        if outputs.shape[2:] != targets.shape[1:]:\n",
    "            targets = torch.nn.functional.interpolate(\n",
    "                targets.unsqueeze(1).float(), size=outputs.shape[2:], mode='nearest'\n",
    "            ).squeeze(1).long()\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_correct += (preds == targets).sum().item()\n",
    "        train_total += targets.numel()\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for voxels, targets in val_loader:\n",
    "            voxels, targets = voxels.to(device), targets.to(device)\n",
    "            outputs = model(voxels)\n",
    "\n",
    "            if outputs.shape[2:] != targets.shape[1:]:\n",
    "                targets = torch.nn.functional.interpolate(\n",
    "                    targets.unsqueeze(1).float(), size=outputs.shape[2:], mode='nearest'\n",
    "                ).squeeze(1).long()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss_sum += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == targets).sum().item()\n",
    "            val_total += targets.numel()\n",
    "\n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    train_loss = train_loss_sum / max(len(train_loader), 1)\n",
    "    val_loss = val_loss_sum / max(len(val_loader), 1)\n",
    "    train_acc = train_correct / max(train_total, 1)\n",
    "    val_acc = val_correct / max(val_total, 1)\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, Path(CONFIG['checkpoint_dir']) / 'voxel3d_best.pth')\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == CONFIG['num_epochs'] - 1:\n",
    "        print(f'Epoch {epoch:3d}/{CONFIG[\"num_epochs\"]} | '\n",
    "              f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | '\n",
    "              f'Time: {elapsed:.1f}s')\n",
    "\n",
    "writer.close()\n",
    "print(f'\\nTraining complete. Best val loss: {best_val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
