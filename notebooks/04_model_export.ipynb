{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Export\n",
    "# AutonomousVehiclePerception/notebooks/04_model_export.ipynb\n",
    "\n",
    "Export trained models to ONNX, TorchScript, and apply torch.compile + quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from src.model.cnn_2d import PerceptionCNN2D\n",
    "from src.model.cnn_3d_voxel import VoxelBackbone3D\n",
    "from src.model.fpn_resnet import FPNDetector\n",
    "from src.model.export import (\n",
    "    export_onnx, export_torchscript,\n",
    "    optimize_with_compile, quantize_dynamic\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "EXPORT_DIR = Path('../exports')\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR = Path('../checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D CNN\n",
    "cnn2d = PerceptionCNN2D(num_classes=9)\n",
    "ckpt_2d = CHECKPOINT_DIR / 'cnn_2d_best.pth'\n",
    "if ckpt_2d.exists():\n",
    "    state = torch.load(ckpt_2d, map_location='cpu', weights_only=True)\n",
    "    cnn2d.load_state_dict(state['model_state_dict'])\n",
    "    print(f'Loaded 2D CNN checkpoint (epoch {state[\"epoch\"]}, val_loss={state[\"val_loss\"]:.4f})')\n",
    "else:\n",
    "    print('No 2D CNN checkpoint found, using random weights')\n",
    "cnn2d.eval()\n",
    "\n",
    "# 3D Voxel CNN\n",
    "cnn3d = VoxelBackbone3D(in_channels=1, num_classes=5)\n",
    "ckpt_3d = CHECKPOINT_DIR / 'voxel3d_best.pth'\n",
    "if ckpt_3d.exists():\n",
    "    state = torch.load(ckpt_3d, map_location='cpu', weights_only=True)\n",
    "    cnn3d.load_state_dict(state['model_state_dict'])\n",
    "    print(f'Loaded 3D CNN checkpoint (epoch {state[\"epoch\"]}, val_loss={state[\"val_loss\"]:.4f})')\n",
    "else:\n",
    "    print('No 3D CNN checkpoint found, using random weights')\n",
    "cnn3d.eval()\n",
    "\n",
    "# FPN-ResNet50\n",
    "fpn = FPNDetector(num_classes=9, pretrained=False)\n",
    "fpn.eval()\n",
    "print('FPN-ResNet50 initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Export\n",
    "\n",
    "Export to ONNX for cross-platform deployment (TensorRT, .NET, Java)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_2d = torch.randn(1, 3, 480, 640)\n",
    "dummy_3d = torch.randn(1, 1, 20, 128, 128)\n",
    "\n",
    "print('=== ONNX Export ===')\n",
    "export_onnx(cnn2d, dummy_2d, EXPORT_DIR / 'cnn_2d.onnx')\n",
    "export_onnx(cnn3d, dummy_3d, EXPORT_DIR / 'cnn_3d_voxel.onnx')\n",
    "export_onnx(fpn, dummy_2d, EXPORT_DIR / 'fpn_resnet.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchScript Export\n",
    "\n",
    "Export to TorchScript for C++ runtime and TorchServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== TorchScript Export ===')\n",
    "export_torchscript(cnn2d, dummy_2d, EXPORT_DIR / 'cnn_2d.pt')\n",
    "export_torchscript(cnn3d, dummy_3d, EXPORT_DIR / 'cnn_3d_voxel.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.compile Optimization\n",
    "\n",
    "Apply graph-mode compilation for GPU inference speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== torch.compile Benchmark ===')\n",
    "\n",
    "# Benchmark original vs compiled\n",
    "model = PerceptionCNN2D(num_classes=9).to(device).eval()\n",
    "test_input = torch.randn(1, 3, 480, 640).to(device)\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        model(test_input)\n",
    "\n",
    "# Original timing\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(20):\n",
    "        start = time.perf_counter()\n",
    "        model(test_input)\n",
    "        times.append((time.perf_counter() - start) * 1000)\n",
    "print(f'Original:  {sum(times)/len(times):.2f} ms avg')\n",
    "\n",
    "# Compiled timing\n",
    "if device.type == 'cuda':\n",
    "    compiled_model = optimize_with_compile(model, mode='reduce-overhead')\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            compiled_model(test_input)\n",
    "\n",
    "    times_compiled = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(20):\n",
    "            start = time.perf_counter()\n",
    "            compiled_model(test_input)\n",
    "            times_compiled.append((time.perf_counter() - start) * 1000)\n",
    "    print(f'Compiled:  {sum(times_compiled)/len(times_compiled):.2f} ms avg')\n",
    "    print(f'Speedup:   {sum(times)/sum(times_compiled):.2f}x')\n",
    "else:\n",
    "    print('torch.compile benchmark skipped (CPU only, best results on GPU)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Quantization (INT8)\n",
    "\n",
    "Apply INT8 quantization for reduced model size and faster CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print('=== INT8 Dynamic Quantization ===')\n",
    "quantized = quantize_dynamic(cnn2d, EXPORT_DIR / 'cnn_2d_int8.pth')\n",
    "\n",
    "# Compare sizes\n",
    "onnx_size = os.path.getsize(EXPORT_DIR / 'cnn_2d.onnx') / (1024 * 1024)\n",
    "pt_size = os.path.getsize(EXPORT_DIR / 'cnn_2d.pt') / (1024 * 1024)\n",
    "\n",
    "print(f'\\nModel sizes:')\n",
    "print(f'  ONNX:        {onnx_size:.1f} MB')\n",
    "print(f'  TorchScript: {pt_size:.1f} MB')\n",
    "\n",
    "# Verify quantized model output\n",
    "with torch.no_grad():\n",
    "    orig_out = cnn2d(dummy_2d)\n",
    "    quant_out = quantized(dummy_2d)\n",
    "    print(f'\\nOriginal output shape:  {orig_out.shape}')\n",
    "    print(f'Quantized output shape: {quant_out.shape}')\n",
    "    print(f'Max output diff: {(orig_out - quant_out).abs().max():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "\n",
    "    print('=== ONNX Validation ===')\n",
    "    onnx_model = onnx.load(str(EXPORT_DIR / 'cnn_2d.onnx'))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print('ONNX model valid âœ“')\n",
    "\n",
    "    # Run inference with ONNX Runtime\n",
    "    session = ort.InferenceSession(str(EXPORT_DIR / 'cnn_2d.onnx'))\n",
    "    ort_input = {'input': dummy_2d.numpy()}\n",
    "    ort_output = session.run(None, ort_input)\n",
    "    print(f'ONNX Runtime output shape: {ort_output[0].shape}')\n",
    "\n",
    "    # Compare with PyTorch\n",
    "    import numpy as np\n",
    "    with torch.no_grad():\n",
    "        pt_output = cnn2d(dummy_2d).numpy()\n",
    "    diff = np.abs(pt_output - ort_output[0]).max()\n",
    "    print(f'Max diff PyTorch vs ONNX Runtime: {diff:.6f}')\n",
    "except ImportError:\n",
    "    print('Install onnx and onnxruntime for validation: pip install onnx onnxruntime')\n",
    "\n",
    "print('\\n=== Export Summary ===')\n",
    "for f in sorted(EXPORT_DIR.glob('*')):\n",
    "    size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "    print(f'  {f.name}: {size_mb:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
